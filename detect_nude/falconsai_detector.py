import torch
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import numpy as np
import logging

logger = logging.getLogger(__name__)

class FalconsaiDetector:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ NSFW –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Falconsai/nsfw_image_detection"""
        try:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            self.processor = AutoImageProcessor.from_pretrained("Falconsai/nsfw_image_detection")
            self.model = AutoModelForImageClassification.from_pretrained("Falconsai/nsfw_image_detection")
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å Falconsai/nsfw_image_detection —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ Falconsai/nsfw_image_detection: {str(e)}")
            raise
    def analyze_image(self, image_path):
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞

        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é

        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"!!!!–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_path}")

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path).convert("RGB")

            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            inputs = self.processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤
            id2label = self.model.config.id2label
            results = {}

            max_prob = 0.0
            max_label = None

            logger.info("üß† NSFW –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
            for idx, prob in enumerate(probs):
                label = id2label[idx]
                prob_value = float(prob)
                results[label] = prob_value
                logger.info(f"{label:<10}: {prob_value:.3f}")

                if prob_value > max_prob:
                    max_prob = prob_value
                    max_label = label

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            nsfw_categories = ['porn', 'sexy', 'hentai']
            safe_categories = ['neutral', 'drawings']

            nsfw_score = sum(results.get(k, 0.0) for k in nsfw_categories)
            safe_score = sum(results.get(k, 0.0) for k in safe_categories)

            return {
                'is_nsfw': max_label in nsfw_categories,
                'nsfw_score': round(nsfw_score, 4),
                'safe_score': round(safe_score, 4),
                'confidence': round(max_prob, 4),
                'details': dict(sorted(results.items(), key=lambda x: x[1], reverse=True))
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return {
                'is_nsfw': False,
                'nsfw_score': 0.0,
                'safe_score': 0.0,
                'confidence': 0.0,
                'details': {},
                'error': str(e)
            }

    def analyze_image_old(self, image_path):
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path)
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            inputs = self.processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]

            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
            id2label = self.model.config.id2label
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = {}
            max_prob = 0
            max_label = None
            
            logger.info("üß† NSFW –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
            for idx, prob in enumerate(probs):
                label = id2label[idx]
                prob_value = float(prob)
                results[label] = prob_value
                logger.info(f"{label:<10}: {prob_value:.3f}")
                
                if prob_value > max_prob:
                    max_prob = prob_value
                    max_label = label
            

            return {
                'is_nsfw': max_label in ['porn', 'sexy', 'hentai'],
                'nsfw_score': float(results.get('nsfw', 0.0)),
                'safe_score': float(results.get('normal', 0.0)),
                'confidence': float(max_prob),
                'details': results
            }

            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return {
                'is_nsfw': False,
                'nsfw_score': 0.0,
                'safe_score': 0.0,
                'confidence': 0.0,
                'details': {},
                'error': str(e)
            } 