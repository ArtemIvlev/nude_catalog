import torch
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
import numpy as np
import logging
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)

class NSFWDetector(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        pass
    
    @abstractmethod
    def analyze_image(self, image_path: str) -> dict:
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        pass

class MarqoNSFWDetector(NSFWDetector):
    """–î–µ—Ç–µ–∫—Ç–æ—Ä NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Marqo"""
    
    MODEL_NAME = "Marqo/nsfw-image-detection-384"
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞"""
        try:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self.model = AutoModelForImageClassification.from_pretrained(
                self.MODEL_NAME,
                trust_remote_code=True
            ).to(self.device)
            
            self.processor = AutoImageProcessor.from_pretrained(
                self.MODEL_NAME,
                trust_remote_code=True
            )
            
            logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.MODEL_NAME} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MarqoNSFWDetector: {str(e)}")
            raise
    
    @property
    def name(self) -> str:
        return self.MODEL_NAME
    
    def analyze_image(self, image_path: str) -> dict:
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(image_path).convert("RGB")
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            inputs = self.processor(images=image, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]

            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
            id2label = self.model.config.id2label
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = {}
            max_prob = 0
            max_label = None
            
            logger.info("üß† NSFW –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
            for idx, prob in enumerate(probs):
                label = id2label[idx]
                prob_value = float(prob)
                results[label] = prob_value
                logger.info(f"{label:<10}: {prob_value:.3f}")
                
                if prob_value > max_prob:
                    max_prob = prob_value
                    max_label = label
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ NSFW
            is_nsfw = False
            nsfw_score = 0.0
            safe_score = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç
            nsfw_classes = ['nsfw', 'porn', 'sexy', 'hentai']
            safe_classes = ['normal', 'safe', 'neutral', 'sfw']
            
            for label, score in results.items():
                if any(nsfw_term in label.lower() for nsfw_term in nsfw_classes):
                    nsfw_score += score
                elif any(safe_term in label.lower() for safe_term in safe_classes):
                    safe_score += score
            
            # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –¥–≤–∞ –∫–ª–∞—Å—Å–∞ (NSFW –∏ SFW), –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –Ω–∞–ø—Ä—è–º—É—é
            if len(results) == 2 and 'nsfw' in results and 'sfw' in results:
                nsfw_score = results['nsfw']
                safe_score = results['sfw']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ NSFW
            is_nsfw = nsfw_score > 0.5
            
            return {
                'is_nsfw': is_nsfw,
                'nsfw_score': nsfw_score,
                'safe_score': safe_score,
                'confidence': max_prob,
                'details': results,
                'model': self.name
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return {
                'is_nsfw': False,
                'nsfw_score': 0.0,
                'safe_score': 0.0,
                'confidence': 0.0,
                'details': {},
                'error': str(e),
                'model': self.name
            }

class NudeNetDetector(NSFWDetector):
    """–î–µ—Ç–µ–∫—Ç–æ—Ä NSFW –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ NudeNet"""
    
    MODEL_NAME = "nudenet"
    
    def __init__(self):
        try:
            from nudenet import NudeDetector
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            self.detector = NudeDetector()
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {self.name}: {str(e)}")
            raise
    
    @property
    def name(self) -> str:
        return self.MODEL_NAME
    
    def analyze_image(self, image_path: str) -> dict:
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NSFW –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            image_path: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            detections = self.detector.detect(image_path)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = {}
            max_prob = 0
            max_label = None
            
            logger.info("üß† NSFW –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:")
            for detection in detections:
                label = detection['class']
                prob_value = float(detection['score'])
                results[label] = prob_value
                logger.info(f"{label:<10}: {prob_value:.3f}")
                
                if prob_value > max_prob:
                    max_prob = prob_value
                    max_label = label
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ NSFW
            is_nsfw = max_prob > 0.5
            
            return {
                'is_nsfw': is_nsfw,
                'nsfw_score': max_prob,
                'safe_score': 1 - max_prob,
                'confidence': max_prob,
                'details': results,
                'model': self.name
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return {
                'is_nsfw': False,
                'nsfw_score': 0.0,
                'safe_score': 0.0,
                'confidence': 0.0,
                'details': {},
                'error': str(e),
                'model': self.name
            } 